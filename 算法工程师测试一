Q1.什么是统计学习，其主要特点是什么，其三要素是什么？
A1：统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，又称为统计机器学习；
特点：以计算机为平台；以数据为对象；以方法为中心；以概率论、统计学、信息论以及最优化理论等为理论依托；目的是实现对数据的预测和分析；
三要素：模型、策略、算法；

Q2：什么是过拟合，什么是正则化？请举例来说明其设计原则思想
A2：过拟合：一个由多个变量（特征）决定的模型，我们通过训练得到一个匹配训练集里非常多数据的预测线，但是这个预测线未能概括模型的大致走势，因为它
过于拟合，通过了所有训练数据，因此是一条很扭曲的曲线，上下波动。但是，这样的曲线千方百计的去拟合训练数据，这样会导致它无法泛化到新的数据样本中，
以至于无法预测新样本价格这就是过拟合。同时如果我们没有足够的数据集（训练集）去约束这个变量过多的模型，那么就会发生过拟合。
正则化：是为了解决过拟合问题提出的办法。正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级。在正则化中，我们加入了惩罚项，为的是让一些
非常小，贡献很小得参数项非常小以至于我们可以忽略他们，从而用更简短的假设表达式去表示预测走势。

Q3：什么是特征选择和提取？
A3：特征选择：基于两个问题：1如何评价一组特征是否有效；2寻优算法
特征提取：其实就是特征变换，通过一个线性变换将高维特征映射到低维空间，说白了就是降维变换

Q4：好的特征标准是什么？请举出当今流行的五种以上图像特征
A4：好的特征标准：数量多，辨别能力好(discriminative power)，局部性好(locality)，robustness，速度快
当前流行的图像特征：LBP SIFT SURT BRISK HARRIS HOG CANNY

Q5：回归问题与分类问题有何区别与联系？试举例说明？
A5:回归和分类区别：当待识别的对象变量是连续的，就是一个回归问题；反之就是一个分类问题
联系：都属于监督学习；
举例：房价与尺寸和房间数的关系，如果房价是连续量，就需要线性回归；反之如果房价取特殊的离散量（house和department），就是一个两类分类问题；

Q6:决策树的优点是什么，决策树的学习通常包括几个步骤？CART(Classification and regression tree)是一种什么样的学习方法，其由几步组成？
A6：决策树的优点：简单，既可以处理连续数值特征，也可以处理离散非数值特征；决策时不需要判别函数，只需要确定树结构的每一层节点特征；
决策树步骤：
1）计算所有样本的熵不纯度；
2）计算候选特征划分的样本熵不纯度；
3）计算每一个候选特征的信息增益或信息增益率；
4）确定根节点特征；
5）同上依次确定出每一层的节点，直到叶节点；
CART算法：思想与ID3和C4.5相同，最大的不同在于，CART在每个节点都采用二分法，即每个节点都只有两个子节点，最后构成的是二叉树；CART就可以用于分类
问题也可以用于回归；

Q7：PCA算法与SVD、LDA算法的区别与联系？
A7：SVD可以获取另一个方向上的主成分，而PCA只能获得单个方向上的主成分；SVD计算伪逆；通过SVD可以得到PCA相同的结果，但是SVD通常比直接使用PCA更稳定；
SVD一般是用来诊断两个场的相关关系的，而PCA是用来提取一个场的主要信息的（即主分量）。两者在具体的实现方法上也有不同，SVD是通过矩阵奇异值分解的方法
分解两个场的协方差矩阵的（两个场的维数不同，不对称），而PCA是通过Jacobi方法分解一个场的协方差矩阵（T'*T).
PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。在上篇文章中便是基于特征值分解的一种解释。特征值和奇异值在大部分人的印
象中，往往是停留在纯粹的数学计算中。而且线性代数或者矩阵论里面，也很少讲任何跟特征值与奇异值有关的应用背景。奇异值分解是一个有着很明显的物理意义的一
种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。
LDA，就是Fisher判别分析，基本和PCA是一对双生子，它们之间的区别就是PCA是一种unsupervised的映射方法而LDA是一种supervised映射方法，都可以实现降维，
但是LDA是将高维映射到一维；

Q8：AdaBoost算法的基本思想和基本算法流程是什么？请举例说明cascade adaboost算法的大致实现过程
A8：AdaBoost基本思想：融合多个弱分类器，来提高分类器的分类性能；

Q9：谈谈你对稀疏表达的理解，试举例说明？
A9：稀疏表达的意义在于降维，但这个降维并非局限于节省空间；在模式识别问题中，往往需要大量的先验知识，而稀疏性是其中最主要的一种先验知识，这一洗属性使得许多病态问题变得不在病态，得以让算法进行下去；另外一种理解就是稀疏表达比较简单，奥卡姆剃刀定律中说过，如果两个模型的解释力相同，选择较简洁的那个，而稀疏表达正好满足这一定律。
现实问题中，很多问题都是稀疏的或近似成稀疏的，如：
1）DL中提取特征；
2）小波变换；
3）模式识别中的特征选择和特征提取；
